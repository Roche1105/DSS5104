{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting House Prices: Pushing the Limit of Linear Regression\n",
    "In this project, we aim to predict house prices using a linear regression model. Through careful feature selection, feature engineering, and regularization, we attempt to maximize the performance of the linear model, approaching the accuracy of non-linear models such as XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('house_dataset_processed.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['price']\n",
    "X = data.drop(columns=['price'])\n",
    "\n",
    "# Log-transform the target\n",
    "y_log = np.log(y)\n",
    "\n",
    "# drop NaNs\n",
    "data_cleaned = pd.concat([X, y_log], axis=1).dropna()\n",
    "\n",
    "X = data_cleaned[X.columns]\n",
    "y_log = data_cleaned[y_log.name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Linear Regression MAPE: 0.2665\n"
     ]
    }
   ],
   "source": [
    "# Basic Linear Regression (Baseline)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "mape_list = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y_log.iloc[train_idx], y_log.iloc[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    \n",
    "    mape_list.append(mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "\n",
    "print(f\"Baseline Linear Regression MAPE: {np.mean(mape_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MAPE: 0.2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/st5225/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.250e+01, tolerance: 1.044e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/st5225/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.322e+01, tolerance: 1.041e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MAPE: 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/st5225/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+01, tolerance: 1.014e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/st5225/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.375e+01, tolerance: 1.032e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/st5225/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.332e+01, tolerance: 1.045e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# 4. Direct Ridge and Lasso on Selected Features\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "lasso_model = Lasso(alpha=0.01)\n",
    "\n",
    "models = {'Ridge Regression': ridge_model, 'Lasso Regression': lasso_model}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    mape_list = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y_log.iloc[train_idx], y_log.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        y_pred = np.exp(y_pred_log)\n",
    "\n",
    "        mape_list.append(mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "\n",
    "    print(f\"{model_name} MAPE: {np.mean(mape_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge(alpha=1.0) applies L2 regularization with moderate strength,\n",
    "# shrinking the coefficients towards zero but not setting them exactly to zero.\n",
    "# This helps in preventing overfitting while retaining all features.\n",
    "\n",
    "# Lasso(alpha=0.01) applies L1 regularization with a small penalty,\n",
    "# encouraging sparsity by setting some coefficients exactly to zero,\n",
    "# effectively performing feature selection along with regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the MAPE of the original linear regression, lasso regression, and ridge regression is approximately around 0.27.\n",
    "There is no significant difference between their predictive performance based on 5-fold cross-validation.\n",
    "Therefore, we proceed to apply Random Forest to perform feature selection based on feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection by random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train random forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y_log)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select important features\n",
    "important_features = feature_importance_df.query('Importance > 0.02')['Feature'].tolist()\n",
    "X_selected = X[important_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_space', 'sqft_living', 'sqft_city', 'lot_zip', 'sqft_zip',\n",
       "       'lot_city', 'sqft_lot', 'sqft_above', 'yr_built'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the selected features, let us build a multiple linear regression model and have a look at the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with Selected Features MAPE: 0.2897\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "mape_list = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_selected):\n",
    "    X_train, X_test = X_selected.iloc[train_idx], X_selected.iloc[test_idx]\n",
    "    y_train, y_test = y_log.iloc[train_idx], y_log.iloc[test_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "\n",
    "    mape_list.append(mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "\n",
    "print(f\"Linear Regression with Selected Features MAPE: {np.mean(mape_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAPE is even larger so we'll try apply feature engineering via cross and polinomial features to capture \n",
    "the potential non-linear relations in our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/8bdxl0bd6713jbn8ynyhtgbc0000gn/T/ipykernel_72345/2197940078.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_selected['sqft_living_total_space'] = X_selected['sqft_living'] * X_selected['total_space']\n",
      "/var/folders/w_/8bdxl0bd6713jbn8ynyhtgbc0000gn/T/ipykernel_72345/2197940078.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_selected['sqft_city_lot_zip'] = X_selected['sqft_city'] * X_selected['lot_zip']\n",
      "/var/folders/w_/8bdxl0bd6713jbn8ynyhtgbc0000gn/T/ipykernel_72345/2197940078.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_selected['sqft_zip_lot_city'] = X_selected['sqft_zip'] * X_selected['lot_city']\n"
     ]
    }
   ],
   "source": [
    "# feature engineering \n",
    "# Cross Features (Interaction Terms)\n",
    "X_selected['sqft_living_total_space'] = X_selected['sqft_living'] * X_selected['total_space']\n",
    "X_selected['sqft_city_lot_zip'] = X_selected['sqft_city'] * X_selected['lot_zip']\n",
    "X_selected['sqft_zip_lot_city'] = X_selected['sqft_zip'] * X_selected['lot_city']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression after Feature Engineering: MAPE = 0.2289\n"
     ]
    }
   ],
   "source": [
    "# model building for ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mape_list = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_poly):\n",
    "    X_train, X_test = X_poly[train_idx], X_poly[test_idx]\n",
    "    y_train, y_test = y_log.iloc[train_idx], y_log.iloc[test_idx]\n",
    "    \n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)  # convert back to price scale\n",
    "    \n",
    "    mape_list.append(mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "\n",
    "print(f\"Ridge Regression after Feature Engineering: MAPE = {np.mean(mape_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regression on Original Features MAPE: 0.1808\n"
     ]
    }
   ],
   "source": [
    "# Benchmark model (XGBoost using original features)\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "mape_list = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y_log.iloc[train_idx], y_log.iloc[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    \n",
    "    mape_list.append(mean_absolute_percentage_error(np.exp(y_test), y_pred))\n",
    "\n",
    "print(f\"XGBoost Regression on Original Features MAPE: {np.mean(mape_list):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost benchmark model was trained using all available features in the dataset, except for the target variable (Price). No feature selection, standardization, or polynomial expansion was applied. This ensures that the model leverages the full information content of the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Comparison\n",
    "\n",
    "The table below summarizes the performance of different models evaluated through 5-fold cross-validation:\n",
    "\n",
    "| Model | Feature Set | MAPE (Actual Price) |\n",
    "|:---|:---|:---|\n",
    "| Baseline Linear Regression | All original features | ~0.2665 |\n",
    "| Ridge Regression (Original Features) | All original features | ~0.2666 |\n",
    "| Lasso Regression (Original Features) | All original features | ~0.2749 |\n",
    "| Linear Regression (Selected Features) | Random Forest Selected Features | ~0.2897 |\n",
    "| Advanced Ridge Regression (After Feature Engineering) | Selected Features + Cross Terms + Polynomial Features | ~0.2289 |\n",
    "| XGBoost Regression (Original Features) | All original features | ~0.1808 |\n",
    "\n",
    "Overall, through feature selection and advanced feature engineering (cross terms and polynomial features), the Ridge Regression model's performance was significantly improved compared to the baseline linear models. Although XGBoost still achieved the best performance, the gap was substantially reduced, demonstrating the effectiveness of pushing the limits of linear regression through careful preprocessing and feature engineering.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st5225",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
